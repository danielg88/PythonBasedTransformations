{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data (df_data, df_dates, attributes, metrics, date_column, suffix):\n",
    "    \"\"\"Joins data and creates transformed columns\n",
    "\n",
    "    df_data = Dataframe with the data you want to transform.\n",
    "    df_dates = Dataframe with auxiliary dates.\n",
    "    attributes and metrics, list of columns in string format.\n",
    "    date_column = String with the name of the column that will be used as reference to all the transformations, ONLY ONE COLUMN and must be Date type.\n",
    "    sufix = sufix to add to the metrics columns.\n",
    "\"\"\"\n",
    "    #Joins data and dates dataframe by the 'suffix' column in the date Dataframe.\n",
    "    #Drops the old date column and renames the new one\n",
    "    #Aggregates the data by the attributes columns.\n",
    "    df_return = df_data.join(df_dates, df_data[date_column] == df_dates[suffix], 'left').select(df_data['*'],df_dates.closing_date)\n",
    "    df_return = df_return.drop(date_column).withColumnRenamed('closing_date', date_column)\n",
    "    agg_metrics = [F.sum(col).alias(col+'_'+suffix) for col in metrics]\n",
    "    df_return = df_return.groupBy(attributes).agg(*agg_metrics)\n",
    "    return df_return\n",
    "\n",
    "def union_agg_data(df_data, transformed_data_aux, metrics_aux):\n",
    "    #Unions 2 dataframes by first adding the same columns to them\n",
    "    #Checks if the dataframes contain the same metrics\n",
    "    #Adds missing metrics to dataframes as 0 columns\n",
    "    #Unions the dataframes by selecting in the transformed dataframe the same columns as the data one.\n",
    "    for metric in metrics_aux:\n",
    "        if metric not in df_data.columns:\n",
    "            df_data = df_data.withColumn(metric,F.lit(0))\n",
    "        if metric not in transformed_data_aux.columns:\n",
    "            transformed_data_aux = transformed_data_aux.withColumn(metric,F.lit(0))\n",
    "    df_data = df_data.union (transformed_data_aux.select(df_data.columns))\n",
    "    return df_data\n",
    "   \n",
    "def mtd_table_generator (min_date, max_date):\n",
    "        df = spark.sql(\"SELECT sequence(to_date('\"+ min_date.strftime(\"%Y-%m-%d\") + \"'), to_date('\"+ max_date.strftime(\"%Y-%m-%d\") + \"'), interval 1 day) as closing_date\").withColumn('closing_date', F.explode(F.col('closing_date')))\n",
    "        df2 = spark.sql(\"SELECT sequence(to_date('\"+ min_date.strftime(\"%Y-%m-%d\") + \"'), to_date('\"+ max_date.strftime(\"%Y-%m-%d\") + \"'), interval 1 day) as mtd\").withColumn('mtd', F.explode(F.col('mtd')))\n",
    "        df = df.crossJoin(df2)\n",
    "        # Filter dataframe keep only dates older than closing date from the beginning of the month.        \n",
    "        df = df.where((df.closing_date>=df.mtd) & (F.date_trunc('month','closing_date')==F.date_trunc('month','mtd')) )\n",
    "        df = df.withColumn('mtd_py', F.date_sub('mtd',365)) #MTD Previous year\n",
    "        return df\n",
    "def ytd_table_generator (min_date, max_date):\n",
    "        df = spark.sql(\"SELECT sequence(to_date('\"+ min_date.strftime(\"%Y-%m-%d\") + \"'), to_date('\"+ max_date.strftime(\"%Y-%m-%d\") + \"'), interval 1 day) as closing_date\").withColumn('closing_date', F.explode(F.col(\"closing_date\")))\n",
    "        df2 = spark.sql(\"SELECT sequence(to_date('\"+ min_date.strftime(\"%Y-%m-%d\") + \"'), to_date('\"+ max_date.strftime(\"%Y-%m-%d\") + \"'), interval 1 day) as ytd\").withColumn('ytd', F.explode(F.col('ytd')))\n",
    "        df = df.crossJoin(df2)\n",
    "        # Filter dataframe keep only dates older than closing date from the beginning of the year.\n",
    "        df = df.where((df.closing_date>=df.ytd) & (F.date_trunc('year','closing_date')==F.date_trunc('year','ytd')) )\n",
    "        df = df.withColumn('ytd_py', F.date_sub('ytd',365)) #YTD Previous year\n",
    "        return df\n",
    "def one_to_one_table_generator (min_date, max_date):\n",
    "        df = spark.sql(\"SELECT sequence(to_date('\"+ min_date.strftime(\"%Y-%m-%d\") + \"'), to_date('\"+ max_date.strftime(\"%Y-%m-%d\") + \"'), interval 1 day) as closing_date\").withColumn('closing_date', F.explode(F.col(\"closing_date\")))\n",
    "        df = df.withColumn('pd', F.date_sub('closing_date',1)) #Previous day\n",
    "        df = df.withColumn('pm', F.add_months(F.col('closing_date'),-1)) #Previous month same day\n",
    "        df = df.withColumn('ld', F.to_date(F.concat((F.extract(F.lit('YEAR'),'closing_date')-1),F.lit('-12-31')),'yyyy-MM-dd')) #Last december\n",
    "        df = df.withColumn('pm_ld', F.last_day(F.add_months(F.col('closing_date'),-1))) #Last day of previous month \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDateTransformations (df_data,attributes:list[str], metrics:list[str], date_column: str, transformations:list[str]):\n",
    "    \"\"\"Apply Date Transformations\n",
    "   \n",
    "    This function receives a Spark Dataframes and returns a new Spark Dataframe with added numerical columns adding date transformations\n",
    "    such as previous month, MTD calculations, YTD calculations.\n",
    "\n",
    "    df_data = Dataframe with the data you want to transform.\n",
    "\n",
    "    attributes = list of columns with descriptive data in string format. Please include the date column too. \n",
    "\n",
    "    metrics = list of columns with numerical data that you want to transform.\n",
    "\n",
    "    date_column = String with the name of the column that will be used as reference to all the transformations, ONLY ONE COLUMN and \n",
    "    the content must be Date type.\n",
    "\n",
    "    transformations = List of transformations to apply, \"MTD\", \"MTD_PY\", \"YTD\", \"YTD_PY\", \"LD\", \"PD\".\n",
    "\"\"\"\n",
    "    max_date = df_data.select(date_column).groupBy().agg({date_column:'max'}).collect()[0][0]\n",
    "    min_date = df_data.select(date_column).groupBy().agg({date_column:'min'}).collect()[0][0]\n",
    "    metrics_aux = metrics.copy()\n",
    "    schema = df_data.schema\n",
    "    transformed_data_aux = spark.createDataFrame([],schema)\n",
    "\n",
    "    for transformation in transformations:\n",
    "        if transformation == 'MTD' or transformation == 'MTD_PY':\n",
    "            mtd_table = mtd_table_generator(min_date,max_date)\n",
    "            if transformation == 'MTD':\n",
    "                transformed_data_aux = transform_data(df_data,mtd_table,attributes,metrics,date_column,'MTD')\n",
    "                metrics_aux.extend([metric+'_MTD' for metric in metrics])\n",
    "            else:\n",
    "                transformed_data_aux = transform_data(df_data,mtd_table,attributes,metrics,date_column,'MTD_PY')\n",
    "                metrics_aux.extend([metric+'_MTD_PY' for metric in metrics])\n",
    "        if transformation == 'YTD' or transformation == 'YTD_PY':\n",
    "            ytd_table = ytd_table_generator(min_date,max_date)\n",
    "            if transformation == 'YTD':\n",
    "                transformed_data_aux = transform_data(df_data,ytd_table,attributes,metrics,date_column,'YTD')\n",
    "                metrics_aux.extend([metric+'_YTD' for metric in metrics])\n",
    "            else:\n",
    "                transformed_data_aux = transform_data(df_data,ytd_table,attributes,metrics,date_column,'YTD_PY')\n",
    "                metrics_aux.extend([metric+'_YTD_PY' for metric in metrics])\n",
    "        if transformation == 'PD':\n",
    "            pd_table = one_to_one_table_generator(min_date,max_date)\n",
    "            transformed_data_aux = transform_data(df_data,pd_table,attributes,metrics,date_column,'PD')\n",
    "            metrics_aux.extend([metric+'_PD' for metric in metrics])\n",
    "        if transformation == 'PM':\n",
    "            pm_table = one_to_one_table_generator(min_date,max_date)\n",
    "            transformed_data_aux = transform_data(df_data,pm_table,attributes,metrics,date_column,'PM')\n",
    "            metrics_aux.extend([metric+'_PD' for metric in metrics])\n",
    "        if transformation == 'LD':\n",
    "            ld_table = one_to_one_table_generator(min_date,max_date)\n",
    "            transformed_data_aux = transform_data(df_data,ld_table,attributes,metrics,date_column,'LD')\n",
    "            metrics_aux.extend([metric+'_LD' for metric in metrics])\n",
    "        if transformation == 'PM_LD':\n",
    "            ld_table = one_to_one_table_generator(min_date,max_date)\n",
    "            transformed_data_aux = transform_data(df_data,ld_table,attributes,metrics,date_column,'PM_LD')\n",
    "            metrics_aux.extend([metric+'_PM_LD' for metric in metrics])\n",
    "\n",
    "      \n",
    "        df_data = union_agg_data(df_data, transformed_data_aux, metrics_aux)\n",
    "    \n",
    "    agg_metrics = [F.sum(col).alias(col) for col in metrics_aux]\n",
    "    df_data = df_data.groupBy(df_data_attributes).agg(*agg_metrics)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-----------+--------+------------------+------------+------------------+------------+------------+---------------+-----------+-----------+--------+-----------+-----------+-----------+-----------+--------------+\n",
      "|Region| Product|      Date|      Price|Quantity|         Price_MTD|Quantity_MTD|         Price_YTD|Quantity_YTD|Price_MTD_PY|Quantity_MTD_PY|   Price_PD|Quantity_PD|Price_LD|Quantity_LD|   Price_PD|Quantity_PD|Price_PM_LD|Quantity_PM_LD|\n",
      "+------+--------+----------+-----------+--------+------------------+------------+------------------+------------+------------+---------------+-----------+-----------+--------+-----------+-----------+-----------+-----------+--------------+\n",
      "| North|   Music|2024-03-03| 786.013758|      43|      1146.6766336|         192|      1146.6766336|         192|         0.0|              0|134.4174636|         74|     0.0|          0|134.4174636|         74|        0.0|             0|\n",
      "| South|Computer|2024-03-01|647.1594121|      93|       647.1594121|          93|       647.1594121|          93|         0.0|              0|        0.0|          0|     0.0|          0|        0.0|          0|        0.0|             0|\n",
      "| North|   Books|2024-03-02|824.2629527|      27|       824.2629527|          27|       824.2629527|          27|         0.0|              0|        0.0|          0|     0.0|          0|        0.0|          0|        0.0|             0|\n",
      "|  East|   Books|2024-03-03|131.2550527|      32|       686.5707087|          50|       686.5707087|          50|         0.0|              0| 555.315656|         18|     0.0|          0| 555.315656|         18|        0.0|             0|\n",
      "| North|   Books|2024-03-03|954.3096495|      54|      1778.5726022|          81|      1778.5726022|          81|         0.0|              0|824.2629527|         27|     0.0|          0|824.2629527|         27|        0.0|             0|\n",
      "| North|Computer|2024-03-01|556.9622863|      65|       556.9622863|          65|       556.9622863|          65|         0.0|              0|        0.0|          0|     0.0|          0|        0.0|          0|        0.0|             0|\n",
      "| South|Computer|2024-03-03|575.5342415|      55|      1393.6764576|         193|      1393.6764576|         193|         0.0|              0| 170.982804|         45|     0.0|          0| 170.982804|         45|        0.0|             0|\n",
      "|  East|   Music|2024-03-01|663.0665039|      94|       663.0665039|          94|       663.0665039|          94|         0.0|              0|        0.0|          0|     0.0|          0|        0.0|          0|        0.0|             0|\n",
      "|  East|   Music|2024-03-02|890.8353521|      73|       1553.901856|         167|       1553.901856|         167|         0.0|              0|663.0665039|         94|     0.0|          0|663.0665039|         94|        0.0|             0|\n",
      "|  East|   Music|2024-03-03|542.7143176|      91|      2096.6161736|         258|      2096.6161736|         258|         0.0|              0|890.8353521|         73|     0.0|          0|890.8353521|         73|        0.0|             0|\n",
      "|  East|   Books|2024-03-02| 555.315656|      18|        555.315656|          18|        555.315656|          18|         0.0|              0|        0.0|          0|     0.0|          0|        0.0|          0|        0.0|             0|\n",
      "| South|   Music|2024-03-03|720.9795626|       4|      1483.8446754|         115|      1483.8446754|         115|         0.0|              0|499.9641315|         90|     0.0|          0|499.9641315|         90|        0.0|             0|\n",
      "| South|   Music|2024-03-01|262.9009813|      21|       262.9009813|          21|       262.9009813|          21|         0.0|              0|        0.0|          0|     0.0|          0|        0.0|          0|        0.0|             0|\n",
      "| South|Computer|2024-03-02| 170.982804|      45|       818.1422161|         138|       818.1422161|         138|         0.0|              0|647.1594121|         93|     0.0|          0|647.1594121|         93|        0.0|             0|\n",
      "| North|   Music|2024-03-02|134.4174636|      74|       360.6628756|         149|       360.6628756|         149|         0.0|              0| 226.245412|         75|     0.0|          0| 226.245412|         75|        0.0|             0|\n",
      "| South|   Books|2024-03-03|216.2406028|      82|        616.957741|         159|        616.957741|         159|         0.0|              0|400.7171382|         77|     0.0|          0|400.7171382|         77|        0.0|             0|\n",
      "|  East|Computer|2024-03-01|362.5919681|      80|       362.5919681|          80|       362.5919681|          80|         0.0|              0|        0.0|          0|     0.0|          0|        0.0|          0|        0.0|             0|\n",
      "| North|Computer|2024-03-03|892.2455727|      97|     1488.57288772|         259|     1488.57288772|         259|         0.0|              0|39.36502872|         97|     0.0|          0|39.36502872|         97|        0.0|             0|\n",
      "|  East|Computer|2024-03-03|72.19181479|      21|1019.7085131899998|         138|1019.7085131899998|         138|         0.0|              0|584.9247303|         37|     0.0|          0|584.9247303|         37|        0.0|             0|\n",
      "| South|   Music|2024-03-02|499.9641315|      90|       762.8651128|         111|       762.8651128|         111|         0.0|              0|262.9009813|         21|     0.0|          0|262.9009813|         21|        0.0|             0|\n",
      "+------+--------+----------+-----------+--------+------------------+------------+------------------+------------+------------+---------------+-----------+-----------+--------+-----------+-----------+-----------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    path = \"/home/jovyan/work/Table.csv\"\n",
    "    \n",
    "    from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, DateType, DoubleType\n",
    "    customSchema = StructType([\n",
    "        StructField('Region', StringType(), True),        \n",
    "        StructField('Product', StringType(), True),\n",
    "        StructField('Date', DateType(), True),\n",
    "        StructField('Quantity', IntegerType(), True),\n",
    "        StructField('Price', StringType(), True) #String because csv uses comma as decimal separator and not supported directly in Spark\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    df_data = spark.read.schema(customSchema).options(delimiter=\";\", header=True).csv(path)\n",
    "    df_data = df_data.withColumn('Price_number', F.regexp_replace('Price', ',', '.').cast(DoubleType())).drop('Price')\n",
    "    df_data = df_data.withColumnRenamed('Price_number','Price')\n",
    "    \n",
    "    \n",
    "    df_data_metrics = ['Price', 'Quantity']\n",
    "    df_data_attributes = ['Region','Product','Date']\n",
    "    df_data_date = 'Date'\n",
    "    \n",
    "    df_transformations = applyDateTransformations (df_data,df_data_attributes, df_data_metrics, df_data_date, ['MTD','YTD','MTD_PY','PD','LD','PM','PM_LD'])\n",
    "    \n",
    "    df_transformations.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
